{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMW97q2e2uLxkHcCNtFiQTD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SashaNasonova/burnSeverity/blob/main/BARC_ImageSearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Burn Severity Mapping Notebook - Image Search\n",
        "This notebook is intended to be used for small scale, interactive burn severity mapping of individual fires in conjunction with the BARC_SBS notebook. For large scale semi-automated mapping please refer to the main python scripts (https://github.com/SashaNasonova/burnSeverity).\n",
        "\n",
        "This notebook checks for all available pre- and post-fire imagery for a fire perimeter (user-defined or downloaded from BC Data Catalogue) and calculate the\n",
        "\n",
        "Steps:\n",
        "1. Clone repository, install packages, and define functions\n",
        "2. Authenticate Google Earth Engine and initialize a Google Cloud project\n",
        "3. Import fire perimeters from https://pub.data.gov.bc.ca/datasets/cdfc2d7b-c046-4bf0-90ac-4897232619e1/prot_current_fire_polys.zip or upload your own perimeters in shapefile format with auxiliary files as well ('/content/perims.shp').\n",
        "4. Select an individual fire number\n",
        "5. Define which sensors to search\n",
        "6. Search for pre-fire imagery by defining a time range (T1 and T2) and maximum scene cloud cover (e.g. less than 70%) and save outputs to a csv.\n",
        "7. Visualize pre-fire imagery in table format\n",
        "8. Search for post-fire imagery by defining a time range (T1 and T2) and maximum scene cloud cover (e.g. less than 70%) and save outputs to a csv.\n",
        "9. Visualize post-fire imagery in a table\n",
        "\n",
        "All outputs are saved to a folder named by date and time (e.g. /content/scene_availability_eval_2025-08-12_17-01-33). Image availability tables are named by fire number (e.g. /content/scene_availability_eval_2025-08-12_18-56-07/V71145_post_all_mosaicMetadata.csv).\n",
        "\n"
      ],
      "metadata": {
        "id": "a7fvXjb6LSn2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4RJK0qJLOQQ"
      },
      "outputs": [],
      "source": [
        "# Clone github repository to be able to access the test data and provincial extent vector data\n",
        "!git clone https://github.com/SashaNasonova/burnSeverity.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the libraries\n",
        "%pip install geemap==0.32.1 #Stable version is (0.32.1) from 15-Jul-2024\n",
        "%pip install pycrs rasterio python-pptx cartopy requests"
      ],
      "metadata": {
        "id": "5VERCksbLdia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the libraries\n",
        "import ee\n",
        "import geemap\n",
        "import os, json, shutil\n",
        "import geopandas\n",
        "from osgeo import gdal\n",
        "from google.colab import files\n",
        "import requests, zipfile\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import warnings\n"
      ],
      "metadata": {
        "id": "-YLtSQffLe5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Processing function\n",
        "# firenumber: string, unique fire perimeter identifier from BC Wildfire\n",
        "# dattypes: list of strings, one or more sensor types\n",
        "# poly: ee.FeatureClass, fire perimeter\n",
        "# T1: string, first date of search interval (eg. 2025-06-01 is June 1st, 2025)\n",
        "# T2: string, last date of search interval, not inclusive\n",
        "# dattype_info: dictionary, sensor types and associated information\n",
        "# outfolder: string, output root folder\n",
        "# timing: string, 'pre' or 'post'\n",
        "\n",
        "def eval(firenumber=None,dattypes=None,poly=None,T1=None,T2=None,dattype_info=None,outfolder=None,timing=None):\n",
        "  warnings.simplefilter(action='ignore', category=FutureWarning) #silencing future warnings\n",
        "  def aoionly(img):\n",
        "    return(img.updateMask(poly_mask))\n",
        "\n",
        "  def getfiles(d,ext):\n",
        "      paths = []\n",
        "      for file in os.listdir(d):\n",
        "          if file.endswith(ext):\n",
        "              paths.append(os.path.join(d, file))\n",
        "      return(paths)\n",
        "\n",
        "  #Helper function must be nested within processing function\n",
        "  def getDate(im):\n",
        "      return(ee.Image(im).date().format(\"YYYY-MM-dd\"))\n",
        "\n",
        "  def getSceneIds(im):\n",
        "      return(ee.Image(im).get('PRODUCT_ID'))\n",
        "\n",
        "  def mosaicByDate(indate):\n",
        "      d = ee.Date(indate)\n",
        "      #print(d)\n",
        "      im = col.filterBounds(poly).filterDate(d, d.advance(1, \"day\")).mosaic()\n",
        "      #print(im)\n",
        "      return(im.set(\"system:time_start\", d.millis(), \"system:index\", d.format(\"YYYY-MM-dd\")))\n",
        "\n",
        "  def runDateMosaic(col_list):\n",
        "      #get a list of unique dates within the list\n",
        "      date_list = col_list.map(getDate).getInfo()\n",
        "      udates = list(set(date_list))\n",
        "      udates.sort()\n",
        "      udates_ee = ee.List(udates)\n",
        "\n",
        "      #mosaic images by unique date\n",
        "      mosaic_imlist = udates_ee.map(mosaicByDate)\n",
        "      return(ee.ImageCollection(mosaic_imlist))\n",
        "\n",
        "  #Landsat cloud mask from metadata\n",
        "  ## Check this!!!\n",
        "  def get_cloud(img1):\n",
        "      ### Change as of Oct 24, 2023: cloud shadow is too inaccurate, remove\n",
        "      ### Though it is picking up topographic shadow. Questions!\n",
        "      # Bits 3 and 4 are cloud and cloud shadow, respectively.\n",
        "      #cloudShadowBitMask = (1 << 4)\n",
        "      cloudBitMask = (1 << 3)\n",
        "      # Get the pixel QA band.\n",
        "      qa = img1.select('QA_PIXEL')\n",
        "      #set both flags to 1\n",
        "      #clouds = qa.bitwiseAnd(cloudBitMask).eq(0).And(qa.bitwiseAnd(cloudShadowBitMask).eq(0)).rename('cloudmsk')\n",
        "      clouds = qa.bitwiseAnd(cloudBitMask).eq(0).rename('cloudmsk')\n",
        "      return(img1.addBands(clouds))\n",
        "\n",
        "  print('Evaluating',firenumber)\n",
        "  df_list = []\n",
        "  for dattype in dattypes:\n",
        "    print(' -Searching',dattype)\n",
        "    col = ee.ImageCollection(dattype_info[dattype]['collection_id']).map(aoionly).select(dattype_info[dattype]['bands'])\n",
        "    cld_field = dattype_info[dattype]['cld_field']\n",
        "    colfilt = col.filterDate(T1,T2).filterBounds(poly).filter(ee.Filter.lt(cld_field,cld))\n",
        "    colfilt_list = colfilt.toList(10000)\n",
        "\n",
        "    if colfilt_list.size().getInfo() == 0:\n",
        "        print('   -Zero scenes were found for',dattype)\n",
        "        continue\n",
        "\n",
        "    # Create before mosaics\n",
        "    mosaic_col = runDateMosaic(colfilt_list)\n",
        "\n",
        "    # Ask server for individual scene metadata\n",
        "    metadata = colfilt.getInfo()\n",
        "\n",
        "    # Turn metadata into table format\n",
        "    features = metadata['features']\n",
        "\n",
        "    out = []\n",
        "    for i in features:\n",
        "        d1 = pd.DataFrame([{'id':i['id']}])\n",
        "        p1 = pd.DataFrame([i['properties']])\n",
        "        t1 = d1.join(p1)\n",
        "        out.append(t1)\n",
        "\n",
        "    meta_df = pd.concat(out)\n",
        "\n",
        "    def strDate(string):\n",
        "        u_str = string.rsplit('_')[1].rsplit('T')[0]\n",
        "        s = u_str[0:4] + '-' + u_str[4:6] + '-' + u_str[6:8]\n",
        "        return(s)\n",
        "\n",
        "    #add date column\n",
        "    if dattype.startswith('S2'):\n",
        "        meta_df['date'] = meta_df['DATATAKE_IDENTIFIER'].apply(strDate)\n",
        "    else:\n",
        "        meta_df['date'] = meta_df['DATE_ACQUIRED']\n",
        "\n",
        "    #outpath = os.path.join(outfolder,firenumber+'_'+dattype+'_'+timing+'_sceneMetadata.csv')\n",
        "    #meta_df.to_csv(outpath)\n",
        "\n",
        "    #make a copy of meta_df\n",
        "    meta_scenes = meta_df.copy()\n",
        "\n",
        "    # Classify to get coverage and cloud extent, fix this to check if any bands are equal to 0\n",
        "    def classify_extent(img1):\n",
        "        if dattype in ['S2','L8_TOA','L9_TOA']:\n",
        "            classes = img1.expression(\"((B2 + B3 + B4) !=0) ? 1 \"\n",
        "                                        \": 0\",{'B2': img1.select('B2'),\n",
        "                                              'B3': img1.select('B3'),\n",
        "                                              'B4': img1.select('B4')}).rename('c').clip(poly)\n",
        "        else:\n",
        "            classes = img1.expression(\"((B2 + B3 + B4) !=0) ? 1 \"\n",
        "                                        \": 0\",{'B2': img1.select('SR_B2'),\n",
        "                                              'B3': img1.select('SR_B3'),\n",
        "                                              'B4': img1.select('SR_B4')}).rename('c').clip(poly)\n",
        "        return(classes)\n",
        "\n",
        "    mosaic_extent = mosaic_col.map(classify_extent).toBands()\n",
        "\n",
        "    def classify_cc(img1):\n",
        "        if dattype.startswith('S2'):\n",
        "            classes = img1.expression(\"(MSK_CLDPRB > 30) ? 1 \"\n",
        "                                \": 0\",{'MSK_CLDPRB': img1.select('MSK_CLDPRB')}).rename('c').clip(poly)\n",
        "        else:\n",
        "            classes = img1.expression(\"(cloudmsk == 1) ? 0 \"\n",
        "                                \": 1\",{'cloudmsk': img1.select('cloudmsk')}).rename('c').clip(poly)\n",
        "        return(classes)\n",
        "\n",
        "    if dattype.startswith('S2'):\n",
        "        mosaic_cc = mosaic_col.map(classify_cc).toBands()\n",
        "        aot = mosaic_col.select('AOT').toBands().divide(1000)\n",
        "        reduced_mean_aot = aot.reduceRegion(reducer=ee.Reducer.mean(),geometry=poly.geometry(),maxPixels=100000000000,scale=30).getInfo()\n",
        "    else:\n",
        "        mosaic_cloudmsk = mosaic_col.map(get_cloud)\n",
        "        mosaic_cc = mosaic_cloudmsk.map(classify_cc).toBands()\n",
        "\n",
        "    #Calculate statistics, if the image is too big this may fail.\n",
        "    #This step causes problems sometimes due to maxPixels limits\n",
        "    reduced_sum = mosaic_extent.reduceRegion(reducer=ee.Reducer.sum(),geometry=poly.geometry(),maxPixels=100000000000,scale=30).getInfo()\n",
        "    reduced_count = mosaic_extent.reduceRegion(reducer=ee.Reducer.count(),geometry=poly.geometry(),maxPixels=100000000000,scale=30).getInfo()\n",
        "\n",
        "    reduced_sum_cc = mosaic_cc.reduceRegion(reducer=ee.Reducer.sum(),geometry=poly.geometry(),maxPixels=100000000000,scale=30).getInfo()\n",
        "    reduced_count_cc = mosaic_cc.reduceRegion(reducer=ee.Reducer.count(),geometry=poly.geometry(),maxPixels=100000000000,scale=30).getInfo()\n",
        "\n",
        "    print('   -Image statistics calculated')\n",
        "\n",
        "    #Rearrange and calculate percent coverage and percent cloud cover\n",
        "    #extent\n",
        "    df_sum = pd.DataFrame([reduced_sum]).T\n",
        "    df_sum.columns = ['sum']\n",
        "\n",
        "    df_count = pd.DataFrame([reduced_count]).T\n",
        "    df_count.columns = ['count']\n",
        "\n",
        "    df_perc = df_sum.join(df_count)\n",
        "    df_perc['percent_coverage'] = (df_perc['sum']/df_perc['count'])*100\n",
        "\n",
        "    #cloud cover\n",
        "    df_sum_cc = pd.DataFrame([reduced_sum_cc]).T\n",
        "    df_sum_cc.columns = ['sum_cc']\n",
        "\n",
        "    df_count_cc = pd.DataFrame([reduced_count_cc]).T\n",
        "    df_count_cc.columns = ['count_cc']\n",
        "\n",
        "    df_perc_cc = df_sum_cc.join(df_count_cc)\n",
        "    df_perc_cc['percent_cc'] = (df_perc_cc['sum_cc']/df_perc_cc['count_cc'])*100\n",
        "    #print(df_perc_cc)\n",
        "\n",
        "    if dattype.startswith('S'):\n",
        "        #aot\n",
        "        df_mean_aot = pd.DataFrame([reduced_mean_aot]).T\n",
        "        df_mean_aot.columns = ['mean_aot']\n",
        "\n",
        "        #join extent and cc\n",
        "        meta_df_ext_temp = df_perc.join(df_perc_cc)\n",
        "\n",
        "        #get rid of cc suffix\n",
        "        oldnames = meta_df_ext_temp.index\n",
        "        newnames = [s.rsplit('_')[0] for s in oldnames]\n",
        "        meta_df_ext_temp.index = newnames\n",
        "\n",
        "        #get rid of aot suffix\n",
        "        oldnames = df_mean_aot.index\n",
        "        newnames = [s.rsplit('_')[0] for s in oldnames]\n",
        "        df_mean_aot.index = newnames\n",
        "\n",
        "        meta_df_ext = meta_df_ext_temp.join(df_mean_aot)\n",
        "        #print(meta_df_ext)\n",
        "\n",
        "    else:\n",
        "        #join extent and cc\n",
        "        meta_df_ext = df_perc.join(df_perc_cc)\n",
        "\n",
        "        #get rid of cc suffix\n",
        "        oldnames = meta_df_ext.index\n",
        "        newnames = [s.rsplit('_')[0] for s in oldnames]\n",
        "        meta_df_ext.index = newnames\n",
        "\n",
        "    #get average scene cloud cover and join to mosaic metadata\n",
        "    meta_scenes_cld = meta_scenes.groupby('date')[cld_field].mean()\n",
        "    temp = pd.DataFrame(meta_scenes_cld)\n",
        "    meta_scenes_cld = temp.rename(columns={'date':'date',cld_field:'percent_cc_scene'})\n",
        "\n",
        "    meta_df_ext = meta_df_ext.join(meta_scenes_cld)\n",
        "    meta_df_ext_out = meta_df_ext.copy().round(2).drop(columns=['sum','count','sum_cc','count_cc'])\n",
        "    meta_df_ext_out['sensor'] = dattype\n",
        "\n",
        "    if 'mean_aot' not in meta_df_ext_out.columns:\n",
        "      meta_df_ext_out['mean_aot'] = None\n",
        "\n",
        "    #outpath = os.path.join(outfolder,firenumber+'_'+dattype+'_'+timing+'_mosaicMetadata.csv')\n",
        "    #meta_df_ext_out.to_csv(outpath)\n",
        "    df_list.append(meta_df_ext_out)\n",
        "    print('   -Dataframe prepared')\n",
        "\n",
        "  #Output final dataframe and save\n",
        "  df_all = pd.concat(df_list)\n",
        "  outpath2 = os.path.join(outfolder,firenumber+'_'+timing+'_all_mosaicMetadata.csv')\n",
        "  df_all.to_csv(outpath2)\n",
        "  print('   -Final dataframe saved to',outpath2)\n",
        "  return(df_all)"
      ],
      "metadata": {
        "id": "1t5KalKSa2sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenticate gee\n",
        "ee.Authenticate()"
      ],
      "metadata": {
        "id": "gvDeGq9rVSsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize with a google cloud project\n",
        "project = 'wlbr-2025'\n",
        "ee.Initialize(project=project)"
      ],
      "metadata": {
        "id": "V1Z8xW_fVWS7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define fire year (options:'current','2024') #added Nov 20, 2025\n",
        "fire_year = 'current'\n",
        "\n",
        "# Get fire perimeter file (either user defined) or pull from BC Wildfire\n",
        "# Open fires shapefile if exists\n",
        "fires_shp = '/content/perims.shp'\n",
        "if os.path.exists(fires_shp):\n",
        "  print('Using user specified perimeter file')\n",
        "else:\n",
        "  if fire_year =='current':\n",
        "    print('Downloading BC Wildfire current fire perimeter file')\n",
        "    url = 'https://pub.data.gov.bc.ca/datasets/cdfc2d7b-c046-4bf0-90ac-4897232619e1/prot_current_fire_polys.zip'\n",
        "    zipname = \"prot_current_fire_poly.zip\"\n",
        "\n",
        "  elif fire_year == '2024':\n",
        "    print('Downloading BC Wildfire historic fire perimeters file (2024)')\n",
        "    url = 'https://coms.api.gov.bc.ca/api/v1/object/8e7ae046-8764-4941-9260-89d2638d1a51'\n",
        "    zipname = \"prot_historic_fire_polys_faib.zip\"\n",
        "\n",
        "  response = requests.get(url)\n",
        "  if response.status_code == 200:\n",
        "      with open(zipname, 'wb') as file:\n",
        "          file.write(response.content)\n",
        "      print(\"File downloaded successfully\")\n",
        "  else:\n",
        "      print(f\"Failed to download file. Status code: {response.status_code}\")\n",
        "\n",
        "  outfolder = '/content/'+zipname.rsplit('.')[0]\n",
        "\n",
        "  with zipfile.ZipFile(zipname, 'r') as zip_ref:\n",
        "      zip_ref.extractall(outfolder)\n",
        "\n",
        "  fires_shp = getfiles(outfolder,'.shp')[0]\n",
        "  print('Fire perimeter file: ',fires_shp)"
      ],
      "metadata": {
        "id": "kQAR7Z8ISQK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()\n",
        "\n",
        "# Visualize in table format\n",
        "fires = geemap.shp_to_ee(fires_shp)\n",
        "fires_df = geopandas.read_file(fires_shp)\n",
        "fires_df_tbl = fires_df.drop(columns=['geometry'], axis=1, inplace=False)\n",
        "#fires_df_tbl = fires_df_tbl[(fires_df_tbl['FIRE_STAT']=='Out') & (fires_df_tbl['FIRE_SZ_HA']>=100)] #uncomment for fires that are out and >= 100 ha\n",
        "fires_df_tbl"
      ],
      "metadata": {
        "id": "4NNoj-NTVlzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select fire number\n",
        "firenumber = 'V71498'"
      ],
      "metadata": {
        "id": "uWZ-LGswTEHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique firenumber field, change if needed\n",
        "fieldname = 'FIRE_NUM'\n",
        "\n",
        "# First check if the firenumber exists in the shapefile provided\n",
        "firelist = fires_df[fieldname].tolist()\n",
        "\n",
        "if firenumber not in firelist:\n",
        "  print('Selected fire number:',firenumber)\n",
        "  print('Available fire numbers: ',firelist)\n",
        "  raise ValueError('Fire number not in fire list. Typo?')\n",
        "\n",
        "# Create output folder\n",
        "outfolder = 'ImageSearch_'+ datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
        "if not os.path.exists(outfolder):\n",
        "  os.mkdir(outfolder)\n",
        "\n",
        "# Save a copy of the fire perimeter\n",
        "vector_folder = os.path.join(outfolder,'vectors')\n",
        "if not os.path.exists(vector_folder):\n",
        "  os.mkdir(vector_folder)\n",
        "\n",
        "outshp = os.path.join(vector_folder,firenumber+'.shp')\n",
        "fires_df_sub = fires_df[fires_df[fieldname]==firenumber]\n",
        "fires_df_sub.to_file(outshp,driver='ESRI Shapefile')\n",
        "\n",
        "# Load in the single perimeter\n",
        "poly = geemap.shp_to_ee(outshp)\n",
        "\n",
        "# Create raster mask to reduce extent of image collections\n",
        "# Function aoionly in functions\n",
        "poly_buf = poly.geometry().buffer(500).bounds()\n",
        "poly_mask = ee.Image.constant(1).clip(poly_buf).selfMask()"
      ],
      "metadata": {
        "id": "YekvDfH_cDTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define collections\n",
        "dattype_info = {\n",
        "    'S2': {\n",
        "        'collection_id': 'COPERNICUS/S2_SR_HARMONIZED',\n",
        "        'cld_field': 'CLOUDY_PIXEL_PERCENTAGE',\n",
        "        'bands': ['B2','B3','B4','MSK_CLDPRB','AOT']\n",
        "    },\n",
        "    'L9': {\n",
        "        'collection_id': 'LANDSAT/LC09/C02/T1_L2',\n",
        "        'cld_field': 'CLOUD_COVER',\n",
        "        'bands':['SR_B2','SR_B3','SR_B4','QA_PIXEL']\n",
        "    },\n",
        "    'L8': {\n",
        "        'collection_id': 'LANDSAT/LC08/C02/T1_L2',\n",
        "        'cld_field': 'CLOUD_COVER',\n",
        "        'bands':['SR_B2','SR_B3','SR_B4','QA_PIXEL']\n",
        "    },\n",
        "    'L8_TOA': {\n",
        "        'collection_id': 'LANDSAT/LC08/C02/T1_TOA',\n",
        "        'cld_field': 'CLOUD_COVER',\n",
        "        'bands':['B2','B3','B4','QA_PIXEL']\n",
        "    },\n",
        "    'L9_TOA': {\n",
        "        'collection_id': 'LANDSAT/LC09/C02/T1_TOA',\n",
        "        'cld_field': 'CLOUD_COVER',\n",
        "        'bands':['B2','B3','B4','QA_PIXEL']\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "95UYaotVY4SO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select which we want to assess\n",
        "dattypes = ['S2','L8_TOA','L9_TOA']"
      ],
      "metadata": {
        "id": "ZyB-scAfqVV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Look for pre-fire imagery\n",
        "timing = 'pre'\n",
        "T1 = '2024-07-01'\n",
        "T2 = '2024-09-01'\n",
        "cld = 40\n",
        "pre_df = eval(firenumber=firenumber,dattypes=dattypes,poly=poly,T1=T1,T2=T2,\n",
        "              dattype_info=dattype_info,outfolder=outfolder,timing=timing)\n",
        "pre_df"
      ],
      "metadata": {
        "id": "klN_VYVhJm1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Look for post-fire imagery\n",
        "timing = 'post'\n",
        "T1 = '2025-08-01'\n",
        "T2 = '2025-09-01'\n",
        "cld = 100\n",
        "post_df = eval(firenumber=firenumber,dattypes=dattypes,poly=poly,T1=T1,T2=T2,\n",
        "              dattype_info=dattype_info,outfolder=outfolder,timing=timing)\n",
        "post_df"
      ],
      "metadata": {
        "id": "For-KMArXbxP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download folder with spreadsheets?\n",
        "zipped = outfolder + '.zip'\n",
        "\n",
        "!zip -r {zipped} {outfolder}\n",
        "files.download(zipped)"
      ],
      "metadata": {
        "id": "TyYfv4rAFHFW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}